{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "rlfQqx0QC5Oa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "4DVEWI2F4UyP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "file_paths = {\n",
        "    \"train_file_path\": {\n",
        "        \"data_file_path\": f\"arc-agi_training_challenges.json\",\n",
        "        \"target_file_path\": f\"arc-agi_training_solutions.json\"\n",
        "    },\n",
        "    \"val_file_path\": {\n",
        "        \"data_file_path\": f\"arc-agi_evaluation_challenges.json\",\n",
        "        \"target_file_path\": f\"arc-agi_evaluation_solutions.json\"\n",
        "    },\n",
        "    \"test_file_path\": {\n",
        "        \"data_file_path\": f\"arc-agi_test_challenges.json\"\n",
        "    },\n",
        "}\n",
        "BATCH_SIZE = 128\n",
        "CMAP = colors.ListedColormap(\n",
        "    ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
        "     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
        "NORM = colors.Normalize(vmin=0, vmax=10)"
      ],
      "metadata": {
        "id": "MYPPL-csKiW8"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ARC Dataset Class\")\n",
        "class ARCDataset:\n",
        "\n",
        "    def __init__(self, train_file_path, val_file_path, test_file_path, batch_size):\n",
        "        self.output = {\n",
        "            \"train_output\":{},\n",
        "            \"val_output\":{}\n",
        "        }\n",
        "        self.origin_data = {}\n",
        "        self.train_data = self.extract_file(train_file_path, \"train\")\n",
        "        self.val_data = self.extract_file(val_file_path, \"val\")\n",
        "        self.test_data = self.extract_file(test_file_path, \"test\")\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    #   for dataset class, we just need the input and output data\n",
        "    def extract_data(self, data):\n",
        "        d = []\n",
        "        for key, inps, targ, index in data:\n",
        "            d.append([inps, targ])\n",
        "        return d\n",
        "\n",
        "    def train_dataset(self):\n",
        "        return DataLoader(self.extract_data(self.train_data), batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataset(self):\n",
        "        return DataLoader(self.extract_data(self.val_data), batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataset(self):\n",
        "        return self.test_data\n",
        "\n",
        "    #   extract json file\n",
        "    def extract_file(self, file_path, type_data):\n",
        "        data_file_path = file_path[\"data_file_path\"]\n",
        "        target_file_path = file_path[\"target_file_path\"] if type_data != \"test\" else None\n",
        "        if target_file_path != None:\n",
        "            with open(target_file_path, 'r') as f:\n",
        "                sol = json.load(f)\n",
        "            for i in sol.keys():\n",
        "                self.output[f\"{type_data}_output\"][i] = sol[i]\n",
        "        return self.load_data(data_file_path, type_data)\n",
        "\n",
        "    def load_data(self, file_path, type_data):\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        self.origin_data[type_data] = data\n",
        "        return self.parse_data(data, type_data)\n",
        "\n",
        "    #   add '0' value for padding. each row must have 30 length\n",
        "    def expand_data(self, data, data_append=0):\n",
        "        return np.array([*data, *[data_append for _ in range(30 - len(data))]])\n",
        "\n",
        "    #   add '0' or np.zeros(30) so the data shape become (30,30) (900 after flatten)\n",
        "    def prep_data(self, data):\n",
        "        data = np.array(data)\n",
        "\n",
        "        ndata = []\n",
        "        for d in data:\n",
        "            ndata.append(self.expand_data(d, 0))\n",
        "        return torch.tensor(self.expand_data(ndata, np.zeros(30)).flatten())\n",
        "\n",
        "    # the input data idea is give the nn example_input + example_target + test_input so LSTM can remember what it should do\n",
        "    def parse_data(self, data, type_data):\n",
        "        ndata = []\n",
        "        for key in tqdm(data.keys(), desc=type_data):\n",
        "            train_data = data[key]['train']\n",
        "            test_data = data[key]['test']\n",
        "            train_temp, test_temp = [], []\n",
        "            for trd in train_data:\n",
        "                input_tensor = self.prep_data(trd['input'])\n",
        "                output_tensor = self.prep_data(trd['output'])\n",
        "                train_temp.append([\n",
        "                    input_tensor,\n",
        "                    output_tensor\n",
        "                ])\n",
        "            for i in range(len(test_data)):\n",
        "                input_tensor = self.prep_data(test_data[i]['input'])\n",
        "                if type_data != 'test' and key in self.output[f\"{type_data}_output\"]:\n",
        "                    output_tensor = self.prep_data(self.output[f\"{type_data}_output\"][key][i])\n",
        "                else:\n",
        "                    output_tensor = np.zeros(900)\n",
        "                test_temp.append([\n",
        "                    input_tensor,\n",
        "                    output_tensor\n",
        "                ])\n",
        "            for i, trd_1 in enumerate(train_temp):\n",
        "                for j, tsd in enumerate(test_temp):\n",
        "                    ndata.append([key, torch.tensor([*[*trd_1[0], 10, *trd_1[1]], 11, *tsd[0], 10]), torch.tensor(tsd[1]), j])\n",
        "\n",
        "        print(f\"Data type: {type_data}. Unique Puzzle: {len(data.keys())}. Parsing Puzzle: {len(ndata)}\")\n",
        "        return ndata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDL6xIUhKWxq",
        "outputId": "f7ade0c1-215d-4b17-bed0-2520e8317df5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC Dataset Class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ARCDataset(**file_paths, batch_size=BATCH_SIZE)\n",
        "train_data = dataset.train_dataset()\n",
        "val_data = dataset.val_dataset()\n",
        "test_data = dataset.test_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2gyhqfAcLYM4",
        "outputId": "80cd16c8-50dd-40dd-d66b-02bb3fec1190"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:   0%|          | 0/400 [00:00<?, ?it/s]<ipython-input-74-71cfb9e0d054>:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  ndata.append([key, torch.tensor([*[*trd_1[0], 10, *trd_1[1]], 11, *tsd[0], 10]), torch.tensor(tsd[1]), j])\n",
            "train: 100%|██████████| 400/400 [00:15<00:00, 25.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: train. Unique Puzzle: 400. Parsing Puzzle: 1392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val: 100%|██████████| 400/400 [00:16<00:00, 24.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: val. Unique Puzzle: 400. Parsing Puzzle: 1456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test: 100%|██████████| 100/100 [00:04<00:00, 23.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: test. Unique Puzzle: 100. Parsing Puzzle: 351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('LSTM Class')\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, INPUT_SIZE, OUTPUT_SIZE, HIDDEN_SIZE):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(INPUT_SIZE, HIDDEN_SIZE, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        lstm_out, _ = self.lstm(input_data)\n",
        "        predictions = self.fc(lstm_out)\n",
        "        return predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3MuIQqdLiLU",
        "outputId": "7ce5c1b0-407b-4df8-c896-bcfe36c41f45"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training Class')\n",
        "class Training:\n",
        "    def __init__(self, model, train_loader, criterion, optimizer, device, loss = 100):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.loss = loss\n",
        "\n",
        "    def _train_one(self, model, data, criterion, optimizer):\n",
        "        # declare model for train mode\n",
        "        model.train()\n",
        "\n",
        "        # data is on cpu, transfer to gpu if gpu is available\n",
        "        input_data, target = data\n",
        "        input_data, target = input_data.to(self.device).float(), target.to(self.device).float()\n",
        "\n",
        "        # get the output\n",
        "        output = model(input_data)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def _train_loop(self, model, train_loader, criterion, optimizer):\n",
        "        model.train()\n",
        "        history = {'train_loss': []}\n",
        "        loss = self.loss\n",
        "        epoch = 0\n",
        "        patient = 0\n",
        "        while True:\n",
        "            epoch += 1\n",
        "            train_loss = 0\n",
        "            for data in train_loader:\n",
        "                ls = self._train_one(model, data, criterion, optimizer)\n",
        "                train_loss += ls\n",
        "            train_loss /= len(train_loader)\n",
        "            history['train_loss'].append(train_loss)\n",
        "\n",
        "            print(f'\\rEpoch : {epoch}, Loss: {train_loss:.5f}, Lowest Loss: {loss:.5f}, Patient: {patient}', end='')\n",
        "\n",
        "            # if loss is smaller than before, save the model\n",
        "            if train_loss < loss:\n",
        "                loss = train_loss\n",
        "                torch.save(model.state_dict(), 'model.pth')\n",
        "                patient = 0\n",
        "            else:\n",
        "                patient += 1\n",
        "            # I'm being greedy here. Sorry. if you dont like it, just remove 'and epoch > 2500'\n",
        "            if patient >= 20:\n",
        "                break\n",
        "\n",
        "        self.loss = loss\n",
        "        return history\n",
        "\n",
        "    def train(self):\n",
        "        history = self._train_loop(self.model, self.train_loader, self.criterion, self.optimizer)\n",
        "        self._plot_loss(history)\n",
        "\n",
        "    def _plot_loss(self, history):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(history['train_loss'], 'o-', label='train_loss')\n",
        "        plt.legend()\n",
        "        plt.title('Loss Plot')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "os-Ux5pRYd_4",
        "outputId": "9153af4d-88b8-452c-b8f0-2b77b3f5ae55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IN_DIM = len(test_data[1][1]) # 2703\n",
        "OUT_DIM = 900\n",
        "LATENT_DIM = 1800"
      ],
      "metadata": {
        "id": "RYlXfOxuZGKE"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training with train data\")\n",
        "model = LSTM(IN_DIM, OUT_DIM, LATENT_DIM).to(DEVICE)\n",
        "criterion = nn.MSELoss()\n",
        "# load pre trained model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "training = Training(model, train_data, criterion, optimizer, DEVICE)\n",
        "training.train()"
      ],
      "metadata": {
        "id": "B-RnQ_2saw1o",
        "outputId": "29b032c7-57a6-4c7f-ae08-262457ed43ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training with train data\n",
            "Epoch : 3, Loss: 1.23266, Lowest Loss: 1.27095, Patient: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwUSSNtUbL8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}