{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "rlfQqx0QC5Oa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4DVEWI2F4UyP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = {\n",
        "    \"train_file_path\": {\n",
        "        \"data_file_path\": f\"arc-agi_training_challenges.json\",\n",
        "        \"target_file_path\": f\"arc-agi_training_solutions.json\"\n",
        "    },\n",
        "    \"val_file_path\": {\n",
        "        \"data_file_path\": f\"arc-agi_evaluation_challenges.json\",\n",
        "        \"target_file_path\": f\"arc-agi_evaluation_solutions.json\"\n",
        "    },\n",
        "    \"test_file_path\": {\n",
        "        \"data_file_path\": f\"arc-agi_test_challenges.json\"\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "MYPPL-csKiW8",
        "outputId": "4c0fe0c9-90ce-458b-f6e7-194b84b01796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BASE_FOLDER' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-3589f46f0558>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m file_paths = {\n\u001b[1;32m      2\u001b[0m     \"train_file_path\": {\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;34m\"data_file_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{BASE_FOLDER}/arc-agi_training_challenges.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"target_file_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{BASE_FOLDER}/arc-agi_training_solutions.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     },\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BASE_FOLDER' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ARC Dataset Class\")\n",
        "class ARCDataset:\n",
        "\n",
        "    def __init__(self, train_file_path, val_file_path, test_file_path, batch_size):\n",
        "        self.output = {\n",
        "            \"train_output\":{},\n",
        "            \"val_output\":{}\n",
        "        }\n",
        "        self.origin_data = {}\n",
        "        self.train_data = self.extract_file(train_file_path, \"train\")\n",
        "        self.val_data = self.extract_file(val_file_path, \"val\")\n",
        "        self.test_data = self.extract_file(test_file_path, \"test\")\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    #   for dataset class, we just need the input and output data\n",
        "    def extract_data(self, data):\n",
        "        d = []\n",
        "        for key, inps, targ, index in data:\n",
        "            d.append([inps, targ])\n",
        "        return d\n",
        "\n",
        "    def train_dataset(self):\n",
        "        return DataLoader(self.extract_data(self.train_data), batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataset(self):\n",
        "        return DataLoader(self.extract_data(self.val_data), batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataset(self):\n",
        "        return self.test_data\n",
        "\n",
        "    #   extract json file\n",
        "    def extract_file(self, file_path, type_data):\n",
        "        data_file_path = file_path[\"data_file_path\"]\n",
        "        target_file_path = file_path[\"target_file_path\"] if type_data != \"test\" else None\n",
        "        if target_file_path != None:\n",
        "            with open(target_file_path, 'r') as f:\n",
        "                sol = json.load(f)\n",
        "            for i in sol.keys():\n",
        "                self.output[f\"{type_data}_output\"][i] = sol[i]\n",
        "        return self.load_data(data_file_path, type_data)\n",
        "\n",
        "    def load_data(self, file_path, type_data):\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        self.origin_data[type_data] = data\n",
        "        return self.parse_data(data, type_data)\n",
        "\n",
        "    #   add '0' value for padding. each row must have 30 length\n",
        "    def expand_data(self, data, data_append=0):\n",
        "        return np.array([*data, *[data_append for _ in range(30 - len(data))]])\n",
        "\n",
        "    #   add '0' or np.zeros(30) so the data shape become (30,30) (900 after flatten)\n",
        "    def prep_data(self, data):\n",
        "        data = np.array(data)\n",
        "\n",
        "        ndata = []\n",
        "        for d in data:\n",
        "            ndata.append(self.expand_data(d, 0))\n",
        "        return torch.tensor(self.expand_data(ndata, np.zeros(30)).flatten())\n",
        "\n",
        "    # the input data idea is give the nn example_input + example_target + test_input so LSTM can remember what it should do\n",
        "    def parse_data(self, data, type_data):\n",
        "        ndata = []\n",
        "        for key in tqdm(data.keys(), desc=type_data):\n",
        "            train_data = data[key]['train']\n",
        "            test_data = data[key]['test']\n",
        "            train_temp, test_temp = [], []\n",
        "            for trd in train_data:\n",
        "                input_tensor = self.prep_data(trd['input'])\n",
        "                output_tensor = self.prep_data(trd['output'])\n",
        "                train_temp.append([\n",
        "                    input_tensor,\n",
        "                    output_tensor\n",
        "                ])\n",
        "            for i in range(len(test_data)):\n",
        "                input_tensor = self.prep_data(test_data[i]['input'])\n",
        "                if type_data != 'test' and key in self.output[f\"{type_data}_output\"]:\n",
        "                    output_tensor = self.prep_data(self.output[f\"{type_data}_output\"][key][i])\n",
        "                else:\n",
        "                    output_tensor = np.zeros(900)\n",
        "                test_temp.append([\n",
        "                    input_tensor,\n",
        "                    output_tensor\n",
        "                ])\n",
        "            for i, trd_1 in enumerate(train_temp):\n",
        "                for j, tsd in enumerate(test_temp):\n",
        "                    ndata.append([key, torch.tensor([*[*trd_1[0], 10, *trd_1[1]], 11, *tsd[0], 10]), torch.tensor(tsd[1]), j])\n",
        "\n",
        "        print(f\"Data type: {type_data}. Unique Puzzle: {len(data.keys())}. Parsing Puzzle: {len(ndata)}\")\n",
        "        return ndata"
      ],
      "metadata": {
        "id": "yDL6xIUhKWxq",
        "outputId": "ea5f6fd4-3c36-4549-e626-92d7c1f0b4ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC Dataset Class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ARCDataset(**file_paths)"
      ],
      "metadata": {
        "id": "2gyhqfAcLYM4",
        "outputId": "2d94900c-0c72-4f0a-f9ca-0a3f725d7e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not dict",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-7a8da119f144>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-a399e462d75c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train, val, test)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-a399e462d75c>\u001b[0m in \u001b[0;36mextract_file\u001b[0;34m(self, file_path, type_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mextract_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not dict"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3MuIQqdLiLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}